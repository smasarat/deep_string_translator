{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "################### IMPORTS ####################### \n",
    "\n",
    "from keras import Sequential, models\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from keras.layers import Embedding, LSTM, RepeatVector, TimeDistributed, Dense\n",
    "from textprocessing.processing import Process\n",
    "from training.tokenize import SentencesTokenizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "################### LOADING DATA AND NORMALIZATION ####################### \n",
    "\n",
    "number_of_records = 10000\n",
    "\n",
    "_process = Process(file_path=\"data/deu_to_eng.txt\")\n",
    "input_text_list = _process.normalize_text()\n",
    "# if dataset is huge, use smaller part of it by regarding the memory and cpu limits, \n",
    "input_text_list = input_text_list[:number_of_records]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "################### SOURCE TOKENIZING #######################\n",
    "source_text_list = [item[1].split() for item in input_text_list]\n",
    "# tokenizer\n",
    "_source_tokenizer = SentencesTokenizer()\n",
    "_source_tokenizer.create_tokenizer(source_text_list)\n",
    "_source_tokenizer.save_tokenizer(file_path=\"./model/ger_tokenizer.pkl\")\n",
    "\n",
    "# encoding\n",
    "x_vector = _source_tokenizer.encode_sequences(list(map(lambda x: \" \".join(x), source_text_list)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "################### TARGET TOKENIZING #######################\n",
    "\n",
    "# creating destination normalizer\n",
    "target_text_list = [item[0].split() for item in input_text_list]\n",
    "_target_tokenizer = SentencesTokenizer()\n",
    "_target_tokenizer.create_tokenizer(target_text_list)\n",
    "_target_tokenizer.save_tokenizer(file_path=\"./model/eng_tokenizer.pkl\")\n",
    "\n",
    "# encoding\n",
    "y_vector = _target_tokenizer.encode_sequences(list(map(lambda x: \" \".join(x), target_text_list)))\n",
    "y_vector = _target_tokenizer.encode_output(y_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Saman\\Desktop\\Repos\\deep_string_translator\\tensor_string_translate\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 10, 256)           911872    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 5, 256)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 5, 256)            525312    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 5, 2234)           574138    \n",
      "=================================================================\n",
      "Total params: 2,536,634\n",
      "Trainable params: 2,536,634\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From C:\\Users\\Saman\\Desktop\\Repos\\deep_string_translator\\tensor_string_translate\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 10000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      " - 21s - loss: 4.1601 - val_loss: 3.3383\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.33831, saving model to ./model/ger_to_eng_model.h5\n",
      "Epoch 2/10\n",
      " - 20s - loss: 3.2896 - val_loss: 3.1695\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.33831 to 3.16954, saving model to ./model/ger_to_eng_model.h5\n",
      "Epoch 3/10\n",
      " - 21s - loss: 3.1481 - val_loss: 3.0271\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.16954 to 3.02714, saving model to ./model/ger_to_eng_model.h5\n",
      "Epoch 4/10\n",
      " - 21s - loss: 2.9861 - val_loss: 2.8332\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.02714 to 2.83325, saving model to ./model/ger_to_eng_model.h5\n",
      "Epoch 5/10\n",
      " - 21s - loss: 2.8079 - val_loss: 2.6601\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.83325 to 2.66014, saving model to ./model/ger_to_eng_model.h5\n",
      "Epoch 6/10\n",
      " - 21s - loss: 2.6223 - val_loss: 2.4511\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.66014 to 2.45115, saving model to ./model/ger_to_eng_model.h5\n",
      "Epoch 7/10\n",
      " - 21s - loss: 2.4282 - val_loss: 2.2496\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.45115 to 2.24964, saving model to ./model/ger_to_eng_model.h5\n",
      "Epoch 8/10\n",
      " - 21s - loss: 2.2416 - val_loss: 2.0679\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.24964 to 2.06787, saving model to ./model/ger_to_eng_model.h5\n",
      "Epoch 9/10\n",
      " - 21s - loss: 2.0724 - val_loss: 1.9127\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.06787 to 1.91270, saving model to ./model/ger_to_eng_model.h5\n",
      "Epoch 10/10\n",
      " - 21s - loss: 1.9225 - val_loss: 1.7731\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.91270 to 1.77310, saving model to ./model/ger_to_eng_model.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25fe143aac8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################## DEEP TRAINING (LSTM) ###############\n",
    "model = Sequential()\n",
    "\n",
    "n_units = 256\n",
    "model.add(Embedding(input_dim=_source_tokenizer.vocab_size, output_dim=n_units,\n",
    "                    input_length=_source_tokenizer.length, mask_zero=True))\n",
    "model.add(LSTM(units=n_units))\n",
    "model.add(RepeatVector(_target_tokenizer.length))\n",
    "model.add(LSTM(n_units, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(_target_tokenizer.vocab_size, activation='softmax')))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "print(model.summary())\n",
    "\n",
    "# now you can access the TensorBoard panel (if you have installed tensorboard)\n",
    "# with command tensorboard --logdir=:./model/graph\n",
    "tensor_board_callback = TensorBoard(log_dir='./model/graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "# save the model for further uses.\n",
    "checkpoint = ModelCheckpoint('./model/ger_to_eng_model.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_vector, y_vector, test_size=0.2, random_state=42)\n",
    "model.fit(x=x_vector, y=y_vector, validation_data=(X_test, y_test), callbacks=[tensor_board_callback, checkpoint],\n",
    "          batch_size=64, epochs=10, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ich bin brillentrager:i am\n",
      "du hast mir gefehlt:you found it\n",
      "It is a simple test with only 10 iterations to validating everything is working correctly. Do train with large number of epochs and bigger dataset, or use pretrained models which are already addressed in repository, to get better results.\n"
     ]
    }
   ],
   "source": [
    "################ TEST WITH USE SENTENCES #####################\n",
    "# load model if you want use pretrained model.\n",
    "model = models.load_model(\"./model/ger_to_eng_model.h5\")\n",
    "\n",
    "sentences_to_check_model = [\"ich bin brillentrager\",\"du hast mir gefehlt\"]\n",
    "\n",
    "list_sentences_to_check_model = _source_tokenizer.encode_sequences(sentences_to_check_model, training=False)\n",
    "translated_sentences = model.predict(list_sentences_to_check_model)\n",
    "for i in range(len(list_sentences_to_check_model)):\n",
    "    print(\"{}:{}\".format(sentences_to_check_model[i],\n",
    "                         _target_tokenizer.decode_sequence(predictions=translated_sentences[i])))\n",
    "\n",
    "print(\"It is a simple test with only 10 iterations to validating everything is working correctly. Do train with large number of epochs and bigger dataset, or use pretrained models which are already addressed in repository, to get better results.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (deep_str_translator)",
   "language": "python",
   "name": "pycharm-de422b17"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
